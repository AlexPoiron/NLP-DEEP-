{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP(DEEP) - Lab07\n",
        "## Auhors\n",
        "- Eliot LECLAIR\n",
        "- Alex POIRON\n",
        "- Tom THIL\n",
        "- Aur√©lien VISENTIN"
      ],
      "metadata": {
        "id": "TlpSEfyGml0K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWudFEJ7kI0p"
      },
      "outputs": [],
      "source": [
        "%pip install beir sentence-transformers hnswlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "3voeepPKm2-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports for Data\n",
        "from beir import util as util_beir\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "\n",
        "#Keep same values\n",
        "from random import seed, sample\n",
        "\n",
        "#Imports for the pre-trained model\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "#Lib for the ANN\n",
        "import hnswlib"
      ],
      "metadata": {
        "id": "wu9WdyzCm2f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data and explore structure"
      ],
      "metadata": {
        "id": "RRTaZhS5m7sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"dbpedia-entity\"\n",
        "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
        "data_path = util_beir.download_and_unzip(url, \"datasets\")\n",
        "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")"
      ],
      "metadata": {
        "id": "F-RXfyvJkWDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of the corpus :\", len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4STK6qEq9Ms",
        "outputId": "291a8cb3-4ec9-4a19-eb56-0bd52380302b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the corpus : 4635922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(corpus.items())[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJIYqCEesxhj",
        "outputId": "ab2b10d9-fd48-48b4-e7f9-a990d64a9a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<dbpedia:Animalia_(book)>',\n",
              "  {'text': \"Animalia is an illustrated children's book by Graeme Base. It was originally published in 1986, followed by a tenth anniversary edition in 1996, and a 25th anniversary edition in 2012. Over three million copies have been sold.   A special numbered and signed anniversary edition was also published in 1996, with an embossed gold jacket.\",\n",
              "   'title': 'Animalia (book)'}),\n",
              " ('<dbpedia:Academy_Award_for_Best_Production_Design>',\n",
              "  {'text': \"The Academy Awards are the oldest awards ceremony for achievements in motion pictures. The Academy Award for Best Production Design recognizes achievement in art direction on a film. The category's original name was Best Art Direction, but was changed to its current name in 2012 for the 85th Academy Awards.  This change resulted from the Art Director's branch of the Academy being renamed the Designer's branch.\",\n",
              "   'title': 'Academy Award for Best Production Design'}),\n",
              " ('<dbpedia:An_American_in_Paris>',\n",
              "  {'text': 'An American in Paris is a jazz-influenced symphonic poem by the American composer George Gershwin, written in 1928. Inspired by the time Gershwin had spent in Paris, it evokes the sights and energy of the French capital in the 1920s and is one of his best-known compositions.Gershwin composed An American in Paris on commission from the conductor Walter Damrosch. He scored the piece for the standard instruments of the symphony orchestra plus celesta, saxophones, and automobile horns.',\n",
              "   'title': 'An American in Paris'})]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(queries.items())[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G_tTcjWmfNv",
        "outputId": "703ff68e-08bb-43da-f533-d18581e56022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('INEX_LD-2009022', 'Szechwan dish food cuisine'),\n",
              " ('INEX_LD-2009039', 'roman architecture'),\n",
              " ('INEX_LD-2009053', 'finland car industry manufacturer saab sisu')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(qrels.items())[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMS-4O6S53su",
        "outputId": "9503f184-fce1-4ec6-800a-e9a75f78766a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('INEX_LD-2009022',\n",
              " {'<dbpedia:Afghan_cuisine>': 0,\n",
              "  '<dbpedia:Akan_cuisine>': 0,\n",
              "  '<dbpedia:Ambuyat>': 0,\n",
              "  '<dbpedia:American_Chinese_cuisine>': 1,\n",
              "  '<dbpedia:Ants_climbing_a_tree>': 2,\n",
              "  '<dbpedia:Baingan_bharta>': 1,\n",
              "  '<dbpedia:Bamischijf>': 0,\n",
              "  '<dbpedia:Black_cardamom>': 0,\n",
              "  '<dbpedia:Brazilian_cuisine>': 0,\n",
              "  '<dbpedia:British_cuisine>': 0,\n",
              "  '<dbpedia:Caribbean_cuisine>': 0,\n",
              "  '<dbpedia:Cellophane_noodles>': 1,\n",
              "  '<dbpedia:Ceviche>': 0,\n",
              "  '<dbpedia:Chana_masala>': 0,\n",
              "  '<dbpedia:Chen_Kenichi>': 1,\n",
              "  '<dbpedia:Chen_Kenmin>': 1,\n",
              "  '<dbpedia:Chicago-style_pizza>': 0,\n",
              "  '<dbpedia:Chicken_(food)>': 0,\n",
              "  '<dbpedia:Chifle>': 0,\n",
              "  '<dbpedia:Chili_oil>': 2,\n",
              "  '<dbpedia:Chinatown,_Los_Angeles>': 0,\n",
              "  '<dbpedia:Chinatown>': 1,\n",
              "  '<dbpedia:Chinese_cuisine>': 2,\n",
              "  '<dbpedia:Churumuri_(food)>': 0,\n",
              "  '<dbpedia:Cookbook>': 0,\n",
              "  '<dbpedia:Cooking>': 0,\n",
              "  '<dbpedia:Couscous>': 0,\n",
              "  '<dbpedia:Cuban_cuisine>': 0,\n",
              "  '<dbpedia:Cuisine>': 0,\n",
              "  '<dbpedia:Cuisine_of_Jharkhand>': 0,\n",
              "  '<dbpedia:Cuisine_of_the_Southern_United_States>': 0,\n",
              "  '<dbpedia:Cuisine_of_the_United_States>': 0,\n",
              "  '<dbpedia:Culture_of_the_Song_dynasty>': 0,\n",
              "  '<dbpedia:Curry>': 0,\n",
              "  '<dbpedia:Dal_dhokli>': 0,\n",
              "  '<dbpedia:Ding_Baozhen>': 0,\n",
              "  '<dbpedia:Dish_(food)>': 0,\n",
              "  '<dbpedia:Doubanjiang>': 1,\n",
              "  '<dbpedia:Drunken_shrimp>': 0,\n",
              "  '<dbpedia:Escabeche>': 0,\n",
              "  '<dbpedia:Fermentation_in_food_processing>': 0,\n",
              "  '<dbpedia:Food_presentation>': 0,\n",
              "  '<dbpedia:Fried_rice>': 0,\n",
              "  '<dbpedia:Fuchsia_Dunlop>': 1,\n",
              "  '<dbpedia:Fufu>': 0,\n",
              "  '<dbpedia:Fuqi_feipian>': 2,\n",
              "  '<dbpedia:Gastronomy>': 0,\n",
              "  \"<dbpedia:General_Tso's_chicken>\": 0,\n",
              "  '<dbpedia:German_cuisine>': 0,\n",
              "  '<dbpedia:Ghanaian_cuisine>': 0,\n",
              "  '<dbpedia:Global_cuisine>': 0,\n",
              "  '<dbpedia:Gondi_dumpling>': 0,\n",
              "  '<dbpedia:Greek_cuisine>': 0,\n",
              "  '<dbpedia:Guizhou_cuisine>': 1,\n",
              "  '<dbpedia:Guoba>': 2,\n",
              "  '<dbpedia:Ham_salad>': 0,\n",
              "  '<dbpedia:Harees>': 0,\n",
              "  '<dbpedia:Hazaragi_cuisine>': 0,\n",
              "  '<dbpedia:History_of_Chinese_cuisine>': 1,\n",
              "  '<dbpedia:Hong_Kong_cuisine>': 0,\n",
              "  '<dbpedia:Hot_and_sour_soup>': 1,\n",
              "  '<dbpedia:Hot_pot>': 2,\n",
              "  '<dbpedia:Hot_sauce>': 1,\n",
              "  '<dbpedia:Huaiyang_cuisine>': 0,\n",
              "  '<dbpedia:Hunan_cuisine>': 0,\n",
              "  '<dbpedia:Indian_Chinese_cuisine>': 1,\n",
              "  '<dbpedia:Indian_Singaporean_cuisine>': 0,\n",
              "  '<dbpedia:Indian_cuisine>': 0,\n",
              "  '<dbpedia:Indonesian_cuisine>': 0,\n",
              "  '<dbpedia:Italian_cuisine>': 0,\n",
              "  '<dbpedia:Kebab>': 0,\n",
              "  '<dbpedia:Khmer_(food)>': 0,\n",
              "  '<dbpedia:Knieperkohl>': 0,\n",
              "  '<dbpedia:Korean_cuisine>': 0,\n",
              "  '<dbpedia:Korean_royal_court_cuisine>': 0,\n",
              "  '<dbpedia:Kung_Pao_chicken>': 1,\n",
              "  '<dbpedia:Latin_American_cuisine>': 0,\n",
              "  '<dbpedia:Lechon>': 0,\n",
              "  '<dbpedia:List_of_Asian_cuisines>': 0,\n",
              "  '<dbpedia:List_of_Chinese_dishes>': 1,\n",
              "  '<dbpedia:List_of_cuisines>': 0,\n",
              "  '<dbpedia:List_of_egg_dishes>': 0,\n",
              "  '<dbpedia:List_of_foods>': 0,\n",
              "  '<dbpedia:List_of_potato_dishes>': 0,\n",
              "  '<dbpedia:Mala_sauce>': 2,\n",
              "  '<dbpedia:Malay_cuisine>': 0,\n",
              "  '<dbpedia:Mang√∫>': 0,\n",
              "  '<dbpedia:Mapo_doufu>': 2,\n",
              "  '<dbpedia:Mexican_cuisine>': 0,\n",
              "  '<dbpedia:National_dish>': 0,\n",
              "  '<dbpedia:Omelette>': 0,\n",
              "  '<dbpedia:Osh_(food)>': 0,\n",
              "  '<dbpedia:Outline_of_cuisines>': 0,\n",
              "  '<dbpedia:Outline_of_food_preparation>': 0,\n",
              "  '<dbpedia:Pao_cai>': 1,\n",
              "  '<dbpedia:Pasta>': 0,\n",
              "  '<dbpedia:Philippine_cuisine>': 0,\n",
              "  '<dbpedia:Pichanga_(dish)>': 0,\n",
              "  '<dbpedia:Pilaf>': 0,\n",
              "  '<dbpedia:Porridge>': 0,\n",
              "  '<dbpedia:Regional_cuisine>': 0,\n",
              "  '<dbpedia:Roast_beef>': 0,\n",
              "  '<dbpedia:Run_down>': 0,\n",
              "  '<dbpedia:Sanna_(dish)>': 0,\n",
              "  '<dbpedia:Seafood_dishes>': 0,\n",
              "  '<dbpedia:Shuizhu>': 2,\n",
              "  '<dbpedia:Shun_Lee_Palace>': 0,\n",
              "  '<dbpedia:Sichuan>': 1,\n",
              "  '<dbpedia:Sichuan_cuisine>': 1,\n",
              "  '<dbpedia:Sichuan_pepper>': 1,\n",
              "  '<dbpedia:Side_dish>': 0,\n",
              "  '<dbpedia:Sinki_(food)>': 1,\n",
              "  '<dbpedia:Staple_food>': 0,\n",
              "  '<dbpedia:Street_food>': 0,\n",
              "  '<dbpedia:Suanla_chaoshou>': 2,\n",
              "  '<dbpedia:Sundanese_cuisine>': 0,\n",
              "  '<dbpedia:Tahini>': 0,\n",
              "  '<dbpedia:Taiwanese_cuisine>': 0,\n",
              "  '<dbpedia:Thai_cuisine>': 0,\n",
              "  '<dbpedia:Tongue>': 0,\n",
              "  '<dbpedia:Traditional_food>': 0,\n",
              "  '<dbpedia:Turkish_cuisine>': 0,\n",
              "  '<dbpedia:Twelve-dish_Christmas_Eve_supper>': 0,\n",
              "  '<dbpedia:Twice_cooked_pork>': 2,\n",
              "  '<dbpedia:Urap>': 0,\n",
              "  '<dbpedia:Vicia_faba>': 1,\n",
              "  '<dbpedia:Wonton>': 1,\n",
              "  '<dbpedia:Zha_cai>': 1,\n",
              "  '<dbpedia:Zhal>': 0,\n",
              "  '<dbpedia:Zhangcha_duck>': 1,\n",
              "  '<dbpedia:Zigong>': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduce the Dataset\n",
        "We will take only documents that are at relevant (>= 1 as value in qrels) and add 100K documents randomly that are not relevant (0 as value in qrels)."
      ],
      "metadata": {
        "id": "lTE2H97jsPT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_relevants(qrels: dict, VALUE_RELEVANT=1) -> set:\n",
        "  \"\"\"\n",
        "    Returns a set of documents's ids that correspond to a relevant value\n",
        "    Args:\n",
        "          - qrels (dict): the qrels\n",
        "          - VALUE_RELEVANT (int): global variable in qrels\n",
        "    Returns:\n",
        "          - docs_ids (list): list of only documents's keys correponding to the relevant value given\n",
        "  \"\"\"\n",
        "  docs_ids = set()\n",
        "  \n",
        "  for pairs in qrels.values():\n",
        "      #Add in the set each key when the value is relevant\n",
        "      docs_ids.update(set([k for k,v in pairs.items() if v >= VALUE_RELEVANT]))\n",
        "  \n",
        "  return docs_ids"
      ],
      "metadata": {
        "id": "9O17nQG_nfXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevants = select_relevants(qrels)"
      ],
      "metadata": {
        "id": "LNcJdmwy3DH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(relevants)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx-U_0ZfDENG",
        "outputId": "566c1711-fb08-48b3-e2b2-a3fb051dcf0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14877"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed(23)\n",
        "\n",
        "def select_irrelevants(relevants: set, corpus: dict, K=100000) -> set:\n",
        "  \"\"\"\n",
        "    Returns a set of documents's ids that correspond to an irrelevant value. This set is\n",
        "    a random sample of K values.\n",
        "    Args:\n",
        "          - relevants (set): Set of relevants keys in the original corpus\n",
        "          - corpus (dict): the original corpus\n",
        "    Returns:\n",
        "          - random_sample (set): the random sample of irrelevants docs\n",
        "  \"\"\"\n",
        "  #To gain speed, we use set properties\n",
        "  all_irrelevants = set(corpus.keys()) - relevants\n",
        "\n",
        "  #We choose randomly 100K values\n",
        "  random_sample = set(sample(list(all_irrelevants), K))\n",
        "  \n",
        "  return  random_sample\n"
      ],
      "metadata": {
        "id": "8ilSxxFQh0Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "irrelevants = select_irrelevants(relevants, corpus)"
      ],
      "metadata": {
        "id": "ft7UwaS-jeVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_reduced_corpus(corpus: dict, keys_to_keep: set) -> dict:\n",
        "  \"\"\"\n",
        "    Get the reduced corpus composed of all relevants documents and 100K irrelevants documents randomly chose.\n",
        "    Args:\n",
        "        - corpus (dict): original corpus\n",
        "        - keys_to_keep (list): list of docs's ids that are relevants\n",
        "    Returns:\n",
        "        - reduced_corpus (dict): new corpus reduced.\n",
        "  \"\"\"\n",
        "  reduced_corpus = corpus\n",
        "  #Keys to delete\n",
        "  to_delete = set(corpus.keys()) - keys_to_keep\n",
        "\n",
        "  for k in to_delete:\n",
        "    del reduced_corpus[k]\n",
        "  \n",
        "  return reduced_corpus"
      ],
      "metadata": {
        "id": "Wjbz3_Ho8XP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys_to_keep = relevants.union(irrelevants)\n",
        "reduced_corpus = get_reduced_corpus(corpus, keys_to_keep)"
      ],
      "metadata": {
        "id": "CPIcYckhaG42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lg = len(reduced_corpus)\n",
        "print(lg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgYhvuoEUrWD",
        "outputId": "c6c8c5b4-b17b-4bc6-bff9-e2b4056c3d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "114877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a reduced corpus, we can work with it.\n",
        "\n",
        "## Creation of the model and embeddings"
      ],
      "metadata": {
        "id": "sZsLIsN7hsso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We recup only the text in our reduced corpus to fill it in the model\n",
        "corpus_text = []\n",
        "for _, val in reduced_corpus.items():\n",
        "  corpus_text.append(val['text'])\n",
        "\n",
        "queries_text = list(queries.values())"
      ],
      "metadata": {
        "id": "ryk5nWBS0pcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('msmarco-distilbert-base-v4')\n",
        "\n",
        "queries_embedding = model.encode(queries_text)\n",
        "corpus_embedding = model.encode(corpus_text)"
      ],
      "metadata": {
        "id": "aUdxVhjOiVR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAP @ 100"
      ],
      "metadata": {
        "id": "jD9dh2Fti2vI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We use semantic search to obtain hits from the embeddings\n",
        "hits = util.semantic_search(queries_embedding, corpus_embedding, top_k=100)"
      ],
      "metadata": {
        "id": "QvDxp7Q86rOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we create a function that define if a document in the hits variable is relevant or not."
      ],
      "metadata": {
        "id": "fUVsGACjrIsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IRRELEVANT_VALUE = 0\n",
        "\n",
        "def is_relevant(index_q: int, hit: dict, qrels: dict, reduced_corpus: dict) -> int:\n",
        "  \"\"\"\n",
        "      Boolean function that returns if a document is relevant or not.\n",
        "  Args:\n",
        "        - index_q (int): Index in the list of hits. This index allow us to get the corresponding \n",
        "          tag of the query at index_q in the queries dict.\n",
        "        - hit (dict): Contains {corpus_id, score}. We use it to obtain the corpus_id variable.\n",
        "        - qrels(dict): queries dict\n",
        "        - reduced_corpus(dict): the reduced corpus\n",
        "  Returns: \n",
        "        True or False\n",
        "  \"\"\"\n",
        "  \n",
        "  query_tag = list(queries.keys())[index_q]\n",
        "\n",
        "  corpus_id = list(hit.values())[0]\n",
        "  corpus_tag = list(reduced_corpus.keys())[corpus_id]\n",
        "\n",
        "  # We need to do it in this way due to the fact that the program crash if we try to get the corpus_id with\n",
        "  # qrels[query_tag][corpus_tag] and corpus_tag isn't in qrels[query_tag]\n",
        "\n",
        "  # We need to check it first and in this condition to check if the value == 0\n",
        "  if corpus_tag not in qrels[query_tag] or qrels[query_tag][corpus_tag] == IRRELEVANT_VALUE:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "ls_8q4rSj9cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_precision(hits: list, qrels: dict, reduced_corpus: dict)-> int:\n",
        "  \"\"\"\n",
        "    Get the MAP@100 for the list of hits.\n",
        "    Args:\n",
        "          - hits (list): hits obtain from the semantic search\n",
        "          - qrels (dict): the qrels dict\n",
        "          - reduced_corpus (dict): the reduced corpus\n",
        "    Returns:\n",
        "          - MAP (float) : the score of MAP (between 0 and 1).\n",
        "  \"\"\"\n",
        "  MAP = 0\n",
        "\n",
        "  for index_q in range(len(hits)):\n",
        "    AP = relevants_docs = nb_docs = 0\n",
        "    \n",
        "    for hit in hits[index_q]:\n",
        "      #Get relevance of the document\n",
        "      relevant = is_relevant(index_q, hit, qrels, reduced_corpus)\n",
        "\n",
        "      #Update indexes and get the current precision\n",
        "      nb_docs += 1\n",
        "      relevants_docs += relevant\n",
        "      prec = relevants_docs / nb_docs\n",
        "\n",
        "      # If the document is relevant, we need to add this precision to the average precision\n",
        "      if relevant == 1:\n",
        "        AP += prec\n",
        "\n",
        "    #Little check to avoid the division by 0\n",
        "    if relevants_docs > 0:\n",
        "      AP /= relevants_docs\n",
        "\n",
        "    MAP += AP\n",
        "   \n",
        "  #Total number of queries  \n",
        "  Q = len(hits)\n",
        "  #Formula of the MAP\n",
        "  return MAP / Q"
      ],
      "metadata": {
        "id": "HZSo2X3FwFVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAP = mean_average_precision(hits, qrels, reduced_corpus)\n",
        "print(\"MAP =\", MAP)"
      ],
      "metadata": {
        "id": "XSAP3qH8xyGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc521af6-4bd4-402e-f0ca-672860ae02fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP = 0.6384498705369226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approximate nearest neighbours"
      ],
      "metadata": {
        "id": "vlbV-fAEYLMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the embeddings size\n",
        "embedding_size = corpus_embedding.shape[1]\n",
        "\n",
        "#Create the index object from the Hnswlib librairy\n",
        "index = hnswlib.Index(space='cosine', dim=embedding_size)\n",
        "#Init it with 2 hyperparameters\n",
        "index.init_index(max_elements=len(corpus_embedding), ef_construction=500, M=64)\n",
        "#Add the corpus embeddings\n",
        "index.add_items(corpus_embedding, list(range(len(corpus_embedding))))"
      ],
      "metadata": {
        "id": "IB9On1x_x2kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We recup the hits\n",
        "corpus_ids, scores = index.knn_query(queries_embedding, k=100)"
      ],
      "metadata": {
        "id": "WxH09HiPdFO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the new hits in the same format as the older ones.\n",
        "hits_ann = []\n",
        "for i in range(len(corpus_ids)):\n",
        "  hit = [{'corpus_id': id, 'score': 1-score} for id, score in zip(corpus_ids[i], scores[i])]\n",
        "  hits_ann.append(hit)"
      ],
      "metadata": {
        "id": "VELzNSHi7t7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We get the MAP@100 from the hits obtained by the ANN way\n",
        "MAP_ANN = mean_average_precision(hits_ann, qrels, reduced_corpus)\n",
        "print(\"MAP with ANN = \" + str(MAP_ANN) + \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJtoj_qFfJub",
        "outputId": "5420be41-9b98-4af0-ac97-33a90a84e032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP with ANN = 0.6351560243791695%\n"
          ]
        }
      ]
    }
  ]
}